{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightning\n",
      "  Downloading pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.1/708.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (4.2.0)\n",
      "Collecting tensorboard>=2.9.1\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-0.10.0-py3-none-any.whl (529 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m529.2/529.2 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.9.* in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (1.23.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (5.4.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (2022.5.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /root/mambaforge/lib/python3.9/site-packages (from pytorch_lightning) (4.64.0)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: aiohttp in /root/mambaforge/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /root/mambaforge/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /root/mambaforge/lib/python3.9/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /root/mambaforge/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (62.3.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-1.2.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.4/123.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.49.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /root/mambaforge/lib/python3.9/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.35.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.12.0-py2.py3-none-any.whl (169 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.8/169.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /root/mambaforge/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.16.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /root/mambaforge/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.11.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/mambaforge/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /root/mambaforge/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /root/mambaforge/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/mambaforge/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /root/mambaforge/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/mambaforge/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /root/mambaforge/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.8.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, pyasn1, werkzeug, tensorboard-data-server, rsa, pyDeprecate, pyasn1-modules, protobuf, oauthlib, grpcio, cachetools, absl-py, torchmetrics, requests-oauthlib, markdown, google-auth, google-auth-oauthlib, tensorboard, pytorch_lightning\n",
      "Successfully installed absl-py-1.2.0 cachetools-5.2.0 google-auth-2.12.0 google-auth-oauthlib-0.4.6 grpcio-1.49.1 markdown-3.4.1 oauthlib-3.2.1 protobuf-3.19.6 pyDeprecate-0.3.2 pyasn1-0.4.8 pyasn1-modules-0.2.8 pytorch_lightning-1.7.7 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 torchmetrics-0.10.0 werkzeug-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /root/mambaforge/lib/python3.9/site-packages (from imbalanced-learn) (1.23.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /root/mambaforge/lib/python3.9/site-packages (from imbalanced-learn) (1.8.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /root/mambaforge/lib/python3.9/site-packages (from imbalanced-learn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /root/mambaforge/lib/python3.9/site-packages (from imbalanced-learn) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/mambaforge/lib/python3.9/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1664520439464,
     "user": {
      "displayName": "Владимир Днепровский",
      "userId": "12512107289711404401"
     },
     "user_tz": -180
    },
    "id": "pzgyJhXAtLzC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Recall\n",
    "import joblib\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3452,
     "status": "ok",
     "timestamp": 1664520444384,
     "user": {
      "displayName": "Владимир Днепровский",
      "userId": "12512107289711404401"
     },
     "user_tz": -180
    },
    "id": "o9N4JbcWudk2"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/notebooks/Dataset/train_dataset_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = df_train[['Easting','Northing','Height','Reflectance']], df_train['Class']\n",
    "sampling_strategy = {4: 300000, 1: 300000, 5: 300000, 64: 300000}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy)\n",
    "x_smote, y_smote = smote.fit_resample(x, y)\n",
    "\n",
    "df_train = pd.concat((x_smote, y_smote), ignore_index=False, axis=1)\n",
    "\n",
    "df_train['Easting_log'], df_train['Northing_log'], df_train['Height_log'], df_train['Reflectance_log'] = np.log10((df_train['Easting'], df_train['Northing'], df_train['Height'], (df_train['Reflectance']+45) ))\n",
    "\n",
    "df_train = df_train.drop(['Easting', 'Northing', 'Height', 'Reflectance'], axis=1)\n",
    "\n",
    "df_train = df_train[[col for col in df_train.columns if col != 'Class'] + ['Class']]\n",
    "\n",
    "df_train.loc[df_train[\"Class\"] == 64, \"Class\"] = 2\n",
    "\n",
    "num_0 = len(df_train.loc[(df_train[\"Class\"] == 0)])\n",
    "num_1 = len(df_train.loc[(df_train[\"Class\"] == 1)])\n",
    "num_2 = len(df_train.loc[(df_train[\"Class\"] == 2)])\n",
    "num_3 = len(df_train.loc[(df_train[\"Class\"] == 3)])\n",
    "num_4 = len(df_train.loc[(df_train[\"Class\"] == 4)])\n",
    "num_5 = len(df_train.loc[(df_train[\"Class\"] == 5)])\n",
    "\n",
    "total_nums = num_0 + num_1 + num_2 + num_3 + num_4 + num_5\n",
    "\n",
    "weight_0 = 1/(num_0/total_nums)/2\n",
    "weight_1 = 1/(num_1/total_nums)/2\n",
    "weight_2 = 1/(num_2/total_nums)/2\n",
    "weight_3 = 1/(num_3/total_nums)/2\n",
    "weight_4 = 1/(num_4/total_nums)/2\n",
    "weight_5 = 1/(num_5/total_nums)/2\n",
    "\n",
    "total_weight = np.array([weight_0, weight_1, weight_2, weight_3, weight_4, weight_5])\n",
    "\n",
    "total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weight = torch.from_numpy(total_weight).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.iloc[:, 0:-1]\n",
    "y = df_train.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "CclE1MBU75LW"
   },
   "outputs": [],
   "source": [
    "polier = PolynomialFeatures(3)\n",
    "X_train = polier.fit_transform(X_train)\n",
    "X_val = polier.transform(X_val)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/notebooks/Scalers/polier_97.gz']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, '/notebooks/Scalers/scaler_97.gz')\n",
    "joblib.dump(polier, '/notebooks/Scalers/polier_97.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4175084, 35)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_FEATURES = len(X_train[1])\n",
    "NUM_CLASSES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftOrdering1DCNN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, sign_size=32, cha_input=16, cha_hidden=32, \n",
    "                 K=2, dropout_input=0.2, dropout_hidden=0.2, dropout_output=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = sign_size*cha_input\n",
    "        sign_size1 = sign_size\n",
    "        sign_size2 = sign_size//2\n",
    "        output_size = (sign_size//4) * cha_hidden\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cha_input = cha_input\n",
    "        self.cha_hidden = cha_hidden\n",
    "        self.K = K\n",
    "        self.sign_size1 = sign_size1\n",
    "        self.sign_size2 = sign_size2\n",
    "        self.output_size = output_size\n",
    "        self.dropout_input = dropout_input\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_output = dropout_output\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(input_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_input)\n",
    "        dense1 = nn.Linear(input_dim, hidden_size, bias=False)\n",
    "        self.dense1 = nn.utils.weight_norm(dense1)\n",
    "\n",
    "        # 1st conv layer\n",
    "        self.batch_norm_c1 = nn.BatchNorm1d(cha_input)\n",
    "        conv1 = conv1 = nn.Conv1d(\n",
    "            cha_input, \n",
    "            cha_input*K, \n",
    "            kernel_size=5, \n",
    "            stride = 1, \n",
    "            padding=2,  \n",
    "            groups=cha_input, \n",
    "            bias=False)\n",
    "        self.conv1 = nn.utils.weight_norm(conv1, dim=None)\n",
    "\n",
    "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = sign_size2)\n",
    "\n",
    "        # 2nd conv layer\n",
    "        self.batch_norm_c2 = nn.BatchNorm1d(cha_input*K)\n",
    "        self.dropout_c2 = nn.Dropout(dropout_hidden)\n",
    "        conv2 = nn.Conv1d(\n",
    "            cha_input*K, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv2 = nn.utils.weight_norm(conv2, dim=None)\n",
    "\n",
    "        # 3rd conv layer\n",
    "        self.batch_norm_c3 = nn.BatchNorm1d(cha_hidden)\n",
    "        self.dropout_c3 = nn.Dropout(dropout_hidden)\n",
    "        conv3 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=3, \n",
    "            stride=1, \n",
    "            padding=1, \n",
    "            bias=False)\n",
    "        self.conv3 = nn.utils.weight_norm(conv3, dim=None)\n",
    "        \n",
    "\n",
    "        # 4th conv layer\n",
    "        self.batch_norm_c4 = nn.BatchNorm1d(cha_hidden)\n",
    "        conv4 = nn.Conv1d(\n",
    "            cha_hidden, \n",
    "            cha_hidden, \n",
    "            kernel_size=5, \n",
    "            stride=1, \n",
    "            padding=2, \n",
    "            groups=cha_hidden, \n",
    "            bias=False)\n",
    "        self.conv4 = nn.utils.weight_norm(conv4, dim=None)\n",
    "\n",
    "        self.avg_po_c4 = nn.AvgPool1d(kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.batch_norm2 = nn.BatchNorm1d(output_size)\n",
    "        self.dropout2 = nn.Dropout(dropout_output)\n",
    "        dense2 = nn.Linear(output_size, output_dim, bias=True)\n",
    "        self.dense2 = nn.utils.weight_norm(dense2)        \n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = nn.functional.celu(self.dense1(x))\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.cha_input, self.sign_size1)\n",
    "\n",
    "        x = self.batch_norm_c1(x)\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "\n",
    "        x = self.ave_po_c1(x)\n",
    "\n",
    "        x = self.batch_norm_c2(x)\n",
    "        x = self.dropout_c2(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x_s = x\n",
    "\n",
    "        x = self.batch_norm_c3(x)\n",
    "        x = self.dropout_c3(x)\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "\n",
    "        x = self.batch_norm_c4(x)\n",
    "        x = self.conv4(x)\n",
    "        x =  x + x_s\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.avg_po_c4(x)\n",
    "\n",
    "        x = self.flt(x)\n",
    "\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense2(x)       \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftOrdering1DCNN(\n",
      "  (batch_norm1): BatchNorm1d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout1): Dropout(p=0.3, inplace=False)\n",
      "  (dense1): Linear(in_features=35, out_features=1024, bias=False)\n",
      "  (batch_norm_c1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
      "  (ave_po_c1): AdaptiveAvgPool1d(output_size=8)\n",
      "  (batch_norm_c2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_c2): Dropout(p=0.3, inplace=False)\n",
      "  (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "  (batch_norm_c3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_c3): Dropout(p=0.3, inplace=False)\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
      "  (batch_norm_c4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
      "  (avg_po_c4): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout2): Dropout(p=0.4, inplace=False)\n",
      "  (dense2): Linear(in_features=256, out_features=6, bias=True)\n",
      "  (loss): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SoftOrdering1DCNN(\n",
    "    input_dim=NUM_FEATURES, \n",
    "    output_dim=NUM_CLASSES, \n",
    "    sign_size=16, \n",
    "    cha_input=64, \n",
    "    cha_hidden=64, \n",
    "    K=2, \n",
    "    dropout_input=0.3, \n",
    "    dropout_hidden=0.3,\n",
    "    dropout_output=0.4\n",
    ")\n",
    "# model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=total_weight).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "      \n",
    "    recall = Recall(average='macro', num_classes=6).to(device)    \n",
    "    acc = recall(y_pred_tags, y_test)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = '/notebooks/Model/model_97.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03da7b5eef745b9aca87148fef8f214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 001: | Train Loss: 0.46727 | Val Loss: 0.82882 | Train Recall:82.674| Val Recall:72.498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c1d87aec1e47988daa490e9ee952b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 002: | Train Loss: 0.10946 | Val Loss: 0.08172 | Train Recall:96.010| Val Recall:97.045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0523ef18a0d40f284cd6685dd61cfae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 003: | Train Loss: 0.07922 | Val Loss: 0.07019 | Train Recall:97.103| Val Recall:97.435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc171be45df2487a9a72df2f61350e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 004: | Train Loss: 0.07186 | Val Loss: 0.06445 | Train Recall:97.369| Val Recall:97.708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29883e1626d480ea296fdb79c04d2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 005: | Train Loss: 0.06740 | Val Loss: 0.06076 | Train Recall:97.552| Val Recall:97.835\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68176b080bc943cf9f4f2d430650f56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 006: | Train Loss: 0.06404 | Val Loss: 0.05892 | Train Recall:97.680| Val Recall:97.884\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67543706008f49d49e0006c97b0ad443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 007: | Train Loss: 0.06125 | Val Loss: 0.05686 | Train Recall:97.780| Val Recall:97.935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b607d1ae8964cfebc42ba56d68b7157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 008: | Train Loss: 0.05936 | Val Loss: 0.05515 | Train Recall:97.838| Val Recall:98.006\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3210978455684ade8e8a01cf89b03b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 009: | Train Loss: 0.05766 | Val Loss: 0.05422 | Train Recall:97.905| Val Recall:98.055\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65667f4dc5d74f7183ce4b68bf351b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 010: | Train Loss: 0.05622 | Val Loss: 0.05391 | Train Recall:97.939| Val Recall:98.031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4aba62945fb4d768fafb4d0bbd49e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 011: | Train Loss: 0.05502 | Val Loss: 0.05188 | Train Recall:97.986| Val Recall:98.102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44ebee48b0541279a6c47a18442b168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 012: | Train Loss: 0.05404 | Val Loss: 0.05149 | Train Recall:98.016| Val Recall:98.094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea5efef7ec94e1d9e53b2624f35d6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 013: | Train Loss: 0.05300 | Val Loss: 0.05114 | Train Recall:98.048| Val Recall:98.124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da70a80b29d4f9c9c5e33bff79ce871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 014: | Train Loss: 0.05236 | Val Loss: 0.04975 | Train Recall:98.062| Val Recall:98.159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf5801442a645578191fcef24ce3da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2039 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: /notebooks/Model/model_97.pth\n",
      "Epoch 015: | Train Loss: 0.05166 | Val Loss: 0.04896 | Train Recall:98.083| Val Recall:98.210\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS+1):    \n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "        for X_train_batch, y_train_batch in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_train_pred = model(X_train_batch)\n",
    "\n",
    "            train_loss = criterion(y_train_pred, y_train_batch)\n",
    "            train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            train_epoch_loss += train_loss.item()\n",
    "            train_epoch_acc += train_acc.item()\n",
    "            # tepoch.set_postfix(loss=train_loss.item(), accuracy=train_acc.item())\n",
    "            \n",
    "        with torch.inference_mode():\n",
    "\n",
    "            val_epoch_loss = 0\n",
    "            val_epoch_acc = 0\n",
    "\n",
    "            model.eval()\n",
    "            for X_val_batch, y_val_batch in val_loader:\n",
    "                X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "                y_val_pred = model(X_val_batch)\n",
    "\n",
    "                val_loss = criterion(y_val_pred, y_val_batch)\n",
    "                val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "\n",
    "                val_epoch_loss += val_loss.item()\n",
    "                val_epoch_acc += val_acc.item()\n",
    "                \n",
    "        if val_loss < val_epoch_loss:\n",
    "            print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "            torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
    "            \n",
    "        loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "        loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "        accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "        accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "    \n",
    "        if epoch % 1 == 0:\n",
    "            print(f'Epoch {epoch+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Recall:{train_epoch_acc/len(train_loader):.3f}| Val Recall:{val_epoch_acc/len(val_loader):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LOAD_PATH = '/notebooks/Model/model_97.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftOrdering1DCNN(\n",
       "  (batch_norm1): BatchNorm1d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (dense1): Linear(in_features=35, out_features=1024, bias=False)\n",
       "  (batch_norm_c1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
       "  (ave_po_c1): AdaptiveAvgPool1d(output_size=8)\n",
       "  (batch_norm_c2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_c2): Dropout(p=0.3, inplace=False)\n",
       "  (conv2): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (batch_norm_c3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout_c3): Dropout(p=0.3, inplace=False)\n",
       "  (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "  (batch_norm_c4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), groups=64, bias=False)\n",
       "  (avg_po_c4): AvgPool1d(kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  (flt): Flatten(start_dim=1, end_dim=-1)\n",
       "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (dense2): Linear(in_features=256, out_features=6, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = SoftOrdering1DCNN(\n",
    "    input_dim=NUM_FEATURES, \n",
    "    output_dim=NUM_CLASSES, \n",
    "    sign_size=16, \n",
    "    cha_input=64, \n",
    "    cha_hidden=64, \n",
    "    K=2, \n",
    "    dropout_input=0.1, \n",
    "    dropout_hidden=0.1,\n",
    "    dropout_output=0.5\n",
    ")\n",
    "model_loaded.load_state_dict(torch.load(f=MODEL_LOAD_PATH))\n",
    "model_loaded.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw = pd.read_csv(\"/notebooks/Dataset/test_dataset_test.csv\")\n",
    "# df_test_raw = df_test_raw.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_raw['Easting_log'], df_test_raw['Northing_log'], df_test_raw['Height_log'], df_test_raw['Reflectance_log'] = np.log10((df_test_raw['Easting'], df_test_raw['Northing'], \n",
    "                                                                                                                   df_test_raw['Height'], (df_test_raw['Reflectance']+45) ))\n",
    "\n",
    "df_test_raw = df_test_raw.drop(['id', 'Easting', 'Northing', 'Height', 'Reflectance'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "polier = joblib.load('/notebooks/Scalers/polier_97.gz')\n",
    "scaler = joblib.load('/notebooks/Scalers/scaler_97.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = polier.transform(df_test_raw)\n",
    "df_test = scaler.transform(df_test)\n",
    "\n",
    "class ClassifierDatasetTest(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data       \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "test_dataset = ClassifierDatasetTest(torch.from_numpy(df_test).float())\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64)\n",
    "\n",
    "y_pred_list = []\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)        \n",
    "        _, y_pred_tags = torch.max(y_test_pred, dim = 1)        \n",
    "        y_pred_list.append(y_pred_tags.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "y_pred_list1 = [a.squeeze().tolist() for a in y_pred_list]\n",
    "y_pred_list2 = list(itertools.chain.from_iterable(y_pred_list1))\n",
    "cols = ['id']\n",
    "df_test1 = pd.read_csv(\"/notebooks/Dataset/test_dataset_test.csv\", usecols=cols)\n",
    "df_test1['Class'] = [a for a in y_pred_list2]\n",
    "df_test1.loc[df_test1[\"Class\"] == 2, \"Class\"] = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1143997\n",
       "3      555846\n",
       "4       55560\n",
       "5       23742\n",
       "1       17792\n",
       "64      12532\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1.to_csv('/notebooks/Dataset/test_check_97.csv', index=False)\n",
    "df_test1['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_add = pd.read_csv(\"/notebooks/Dataset/test_check_77.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1151008\n",
       "3      552889\n",
       "4       53300\n",
       "5       23757\n",
       "1       17362\n",
       "64      11153\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_add['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
